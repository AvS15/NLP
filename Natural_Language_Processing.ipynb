{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the first sentences of Barack Obama's inauguration speech.\n",
    "\n",
    "obama_speech = \"\"\"I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors.\n",
    "\n",
    "I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. \n",
    "\n",
    "At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors.\n",
      "\n",
      "I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. \n",
      "\n",
      "At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents."
     ]
    }
   ],
   "source": [
    "# better representation of the text\n",
    "\n",
    "for word in obama_speech:\n",
    "    print(word, sep='', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speech is of type:\t<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# What data type is the text?\n",
    "\n",
    "typ = type(obama_speech)\n",
    "print(\"The speech is of type:\\t{}\".format(typ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', ',', 'grateful', 'for', 'the', 'trust', 'you', 'have', 'bestowed', ',', 'mindful', 'of', 'the', 'sacrifices', 'borne', 'by', 'our', 'ancestors', '.', 'I', 'thank', 'President', 'Bush', 'for', 'his', 'service', 'to', 'our', 'nation', ',', 'as', 'well', 'as', 'the', 'generosity', 'and', 'cooperation', 'he', 'has', 'shown', 'throughout', 'this', 'transition', '.', 'Forty-four', 'Americans', 'have', 'now', 'taken', 'the', 'presidential', 'oath', '.', 'The', 'words', 'have', 'been', 'spoken', 'during', 'rising', 'tides', 'of', 'prosperity', 'and', 'the', 'still', 'waters', 'of', 'peace', '.', 'Yet', ',', 'every', 'so', 'often', 'the', 'oath', 'is', 'taken', 'amidst', 'gathering', 'clouds', 'and', 'raging', 'storms', '.', 'At', 'these', 'moments', ',', 'America', 'has', 'carried', 'on', 'not', 'simply', 'because', 'of', 'the', 'skill', 'or', 'vision', 'of', 'those', 'in', 'high', 'office', ',', 'but', 'because', 'We', 'the', 'People', 'have', 'remained', 'faithful', 'to', 'the', 'ideals', 'of', 'our', 'forbearers', ',', 'and', 'true', 'to', 'our', 'founding', 'documents', '.']\n",
      "\n",
      "Number of single tokens in Obama's speech:\t139\n",
      "Whole Number of characters in Obama's speech:\t739\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text and return every token\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(obama_speech)\n",
    "print(tokens)\n",
    "print()\n",
    "# representation as numpy-array: tokens = np.array(tokens)\n",
    "\n",
    "number_of_tokens = len(tokens)\n",
    "print(\"Number of single tokens in Obama's speech:\\t\" + str(number_of_tokens))\n",
    "print(\"Whole Number of characters in Obama's speech:\\t\" + str(len(obama_speech)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 94 samples and 139 outcomes>\n",
      "Frequency of token 'america':\t1\n",
      "The top 5 are:\t [('the', 11), (',', 7), ('of', 6), ('.', 6), ('have', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Dealing with frequencies\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq = FreqDist()                     # create object\n",
    "\n",
    "for word in tokens:\n",
    "    freq[word.lower()] += 1\n",
    "\n",
    "print(freq)                          # prints details of the FreqDist object\n",
    "\n",
    "freq                                 # prints FreqDist dictionary with tokens (keys) und their frequency (value)\n",
    "\n",
    "\n",
    "print(\"Frequency of token 'america':\\t\" + str(freq['america']))\n",
    "\n",
    "\n",
    "top5_tokens = freq.most_common(5)\n",
    "print(\"The top 5 are:\\t\", str(top5_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blanklines:\t3\n"
     ]
    }
   ],
   "source": [
    "# Dealing with blanklines\n",
    "\n",
    "from nltk.tokenize import blankline_tokenize      \n",
    "\n",
    "blank = blankline_tokenize(obama_speech)\n",
    "\n",
    "print(\"Number of blanklines:\\t\" + str(len(blank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition. Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      "[('God', 'bless'), ('bless', 'you'), ('you', 'and'), ('and', 'God'), ('God', 'bless'), ('bless', 'the'), ('the', 'United'), ('United', 'States'), ('States', 'of'), ('of', 'America'), ('America', '.')]\n",
      "\n",
      "Trigrams:\n",
      "[('God', 'bless', 'you'), ('bless', 'you', 'and'), ('you', 'and', 'God'), ('and', 'God', 'bless'), ('God', 'bless', 'the'), ('bless', 'the', 'United'), ('the', 'United', 'States'), ('United', 'States', 'of'), ('States', 'of', 'America'), ('of', 'America', '.')]\n",
      "\n",
      "5grams:\n",
      "[('God', 'bless', 'you', 'and', 'God'), ('bless', 'you', 'and', 'God', 'bless'), ('you', 'and', 'God', 'bless', 'the'), ('and', 'God', 'bless', 'the', 'United'), ('God', 'bless', 'the', 'United', 'States'), ('bless', 'the', 'United', 'States', 'of'), ('the', 'United', 'States', 'of', 'America'), ('United', 'States', 'of', 'America', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Dealing with bigrams, trigrams and ngrams\n",
    "\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "last_sentence = \"God bless you and God bless the United States of America.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(last_sentence)\n",
    "\n",
    "\n",
    "# bigram: Tokens of two consecutive written words\n",
    "bigrams = list(nltk.bigrams(tokens))\n",
    "print(\"Bigrams:\")\n",
    "print(bigrams)\n",
    "print()\n",
    "\n",
    "\n",
    "# trigram: Tokens of three consecutive written words\n",
    "print(\"Trigrams:\")\n",
    "trigrams = list(nltk.trigrams(tokens))\n",
    "print(trigrams)\n",
    "print()\n",
    "\n",
    "\n",
    "# ngram: Tokens of any number of consecutive written words\n",
    "print(\"5grams:\")\n",
    "ngrams = list(nltk.ngrams(tokens, 5))\n",
    "print(ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WordList(['God']),\n",
       " WordList(['bless']),\n",
       " WordList(['you']),\n",
       " WordList(['and']),\n",
       " WordList(['God']),\n",
       " WordList(['bless']),\n",
       " WordList(['the']),\n",
       " WordList(['United']),\n",
       " WordList(['States']),\n",
       " WordList(['of']),\n",
       " WordList(['America'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TextBlob for generating N-Grams\n",
    "\n",
    "#!pip install -U textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "unigram = TextBlob(last_sentence).ngrams(1)                 # unigram with n = 1\n",
    "unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['God', 'bless']),\n",
       " WordList(['bless', 'you']),\n",
       " WordList(['you', 'and']),\n",
       " WordList(['and', 'God']),\n",
       " WordList(['God', 'bless']),\n",
       " WordList(['bless', 'the']),\n",
       " WordList(['the', 'United']),\n",
       " WordList(['United', 'States']),\n",
       " WordList(['States', 'of']),\n",
       " WordList(['of', 'America'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = TextBlob(last_sentence).ngrams(2)                  # bigram with n = 2\n",
    "bigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giving: give\n",
      "given: given\n",
      "gave: gave\n"
     ]
    }
   ],
   "source": [
    "# Stemming: Normalize words into its base form\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = [\"give\", \"giving\", \"given\", \"gave\"]\n",
    "for w in words:\n",
    "    print(w + \": \" + ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: giv\n",
      "giving: giv\n",
      "given: giv\n",
      "gave: gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(w + \": \" + lst.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giving: give\n",
      "given: given\n",
      "gave: gave\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "sbs = SnowballStemmer('english')\n",
    "\n",
    "for w in words:\n",
    "    print(w + \": \" + sbs.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('God', 'NNP')]\n",
      "[('bless', 'NN')]\n",
      "[('you', 'PRP')]\n",
      "[('and', 'CC')]\n",
      "[('God', 'NNP')]\n",
      "[('bless', 'NN')]\n",
      "[('the', 'DT')]\n",
      "[('United', 'NNP')]\n",
      "[('States', 'NNS')]\n",
      "[('of', 'IN')]\n",
      "[('America', 'NNP')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS: Parts of Speech\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "for t in tokens:                 # remember: tokens = nltk.word_tokenize(last_sentence)\n",
    "    print(nltk.pos_tag([t]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [Errno\n",
      "[nltk_data]     11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading words: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (FACILITY White/NNP House/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# NER: Named Entity Recognition\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk import ne_chunk\n",
    "\n",
    "string = \"The US President stays in the White House.\"\n",
    "\n",
    "tokens = word_tokenize(string)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "ner = ne_chunk(tags)\n",
    "\n",
    "print(ner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'our': 18, 'nation': 14, 'is': 13, 'at': 3, 'war': 26, 'against': 0, 'far': 8, 'reaching': 20, 'network': 15, 'of': 16, 'violence': 25, 'and': 2, 'hatred': 11, 'economy': 7, 'badly': 4, 'weakened': 27, 'consequence': 6, 'greed': 10, 'irresponsibility': 12, 'on': 17, 'the': 23, 'part': 19, 'some': 21, 'god': 9, 'bless': 5, 'you': 28, 'united': 24, 'states': 22, 'america': 1}\n",
      "\n",
      "['against', 'america', 'and', 'at', 'badly', 'bless', 'consequence', 'economy', 'far', 'god', 'greed', 'hatred', 'irresponsibility', 'is', 'nation', 'network', 'of', 'on', 'our', 'part', 'reaching', 'some', 'states', 'the', 'united', 'violence', 'war', 'weakened', 'you']\n",
      "\n",
      "[[1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0]\n",
      " [0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 2 1 1 1 0 1 0 1 0 0 0 1 0]\n",
      " [0 1 1 0 0 2 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer: Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent_1 = \"Our nation is at war, against a far-reaching network of violence and hatred.\"\n",
    "sent_2 = \"Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some.\"\n",
    "sent_3 = \"God bless you and God bless the United States of America.\"\n",
    " \n",
    "msg = [sent_1, sent_2, sent_3]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(msg)                             # tokenizing\n",
    "vector = cv.transform(msg)              # encoding\n",
    "\n",
    "print(cv.vocabulary_)\n",
    "print()\n",
    "print(cv.get_feature_names())\n",
    "print()\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 13)\n",
      "\n",
      "[[ 0.         -0.2        -0.2         0.2         0.2         0.8\n",
      "   0.         -0.2        -0.2         0.         -0.2         0.2\n",
      "  -0.2       ]\n",
      " [ 0.62554324  0.         -0.20851441 -0.20851441  0.20851441  0.20851441\n",
      "   0.20851441  0.          0.         -0.20851441  0.          0.41702883\n",
      "  -0.41702883]\n",
      " [-0.25819889 -0.51639778  0.         -0.51639778  0.          0.51639778\n",
      "   0.          0.          0.          0.          0.25819889  0.25819889\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# HashingVectorizer: Convert a collection of text documents to a matrix of token occurrences.\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "msg = [sent_1, sent_2, sent_3]\n",
    "\n",
    "hv = HashingVectorizer(n_features = 13)     # vector size\n",
    "hashVec = hv.transform(msg)\n",
    "\n",
    "print(hashVec.shape)                        # dimensions\n",
    "print()\n",
    "print(hashVec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'our': 18, 'nation': 14, 'is': 13, 'at': 3, 'war': 26, 'against': 0, 'far': 8, 'reaching': 20, 'network': 15, 'of': 16, 'violence': 25, 'and': 2, 'hatred': 11, 'economy': 7, 'badly': 4, 'weakened': 27, 'consequence': 6, 'greed': 10, 'irresponsibility': 12, 'on': 17, 'the': 23, 'part': 19, 'some': 21, 'god': 9, 'bless': 5, 'you': 28, 'united': 24, 'states': 22, 'america': 1}\n",
      "\n",
      "[1.69314718 1.69314718 1.         1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.28768207 1.69314718 1.69314718 1.         1.69314718\n",
      " 1.28768207 1.69314718 1.69314718 1.69314718 1.69314718 1.28768207\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718]\n"
     ]
    }
   ],
   "source": [
    "# TD-IDF (Term frequency-Inverse Document Frequency): Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "msg = [sent_1, sent_2, sent_3]\n",
    "\n",
    "tdidf = TfidfVectorizer()\n",
    "tdidf.fit(msg)\n",
    "\n",
    "print(tdidf.vocabulary_)\n",
    "print()\n",
    "print(tdidf.idf_)                           # idf=log(N/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing text data with Apache Spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local\") \\\n",
    "            .appName(\"Tokenize Text Data\") \\\n",
    "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "\n",
    "# Create Spark DataFrame\n",
    "data = [(1, \"The people who are crazy enough\"),(2, \"to think they can change the world\"), (3, \"are the one who do.\")]\n",
    "qdf = spark.createDataFrame(data, [\"id\", \"quote\"])\n",
    "qdf.show()\n",
    "qdf.take(1)\n",
    "\n",
    "\n",
    "# Using Tokenizer\n",
    "qdf_token = Tokenizer(inputCol=\"quote\", outputCol=\"words\")\n",
    "qdf_token_df = qdf_token.transform(qdf)\n",
    "qdf_token_df.show()\n",
    "\n",
    "\n",
    "# Using HashingTF (= hashing term frequency)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"htf_features\", numFeatures=20)\n",
    "htf_df = hashingTF.transform(qdf_token_df)\n",
    "htf_df.show()\n",
    "htf_df.take(1)\n",
    "\n",
    "\n",
    "# Using IDF (= inverse document frequency transformation)\n",
    "idf = IDF(inputCol=\"htf_features\", outputCol=\"idf_features\")\n",
    "idf_model = idf.fit(htf_df)\n",
    "idf_df = idf_model.transform(htf_df)\n",
    "idf_df.show()\n",
    "idf_df.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in a pdf file\n",
    "\n",
    "!pip install PyPDF2\n",
    "\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "\n",
    "\n",
    "pdf = open(r\"your_pdf_file.pdf\",\"rb\")               # PDF file object\n",
    "\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf)              # PDF reader object\n",
    "\n",
    "print(pdf_reader.numPages)\n",
    "\n",
    "page = pdf_reader.getPage(1)                        # page object\n",
    "\n",
    "print(page.extractText())                           # extract the text\n",
    "\n",
    "len(re.findall(r\"the\", pdf))                        # Count number of times \"the\" is appeared in the file\n",
    "\n",
    "pdf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
